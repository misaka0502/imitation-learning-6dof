{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing module 'gym_38' (/home/larsankile/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)\n",
      "Setting GYM_USD_PLUG_INFO_PATH to /home/larsankile/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larsankile/miniconda3/envs/rlgpu/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.animation as animation\n",
    "from datetime import datetime\n",
    "\n",
    "import furniture_bench\n",
    "from furniture_bench.envs.observation import DEFAULT_STATE_OBS\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.schedulers.scheduling_ddim import DDIMScheduler\n",
    "\n",
    "# from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "import wandb\n",
    "\n",
    "import zarr\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.0.1+cu117\n",
      "Device count 2\n",
      "/home/larsankile/isaacgym/python/isaacgym/_bindings/src/gymtorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/larsankile/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/larsankile/.cache/torch_extensions/py38_cu117/gymtorch/build.ninja...\n",
      "Building extension module gymtorch...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module gymtorch...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not connected to PVD\n",
      "+++ Using GPU PhysX\n",
      "Physics Engine: PhysX\n",
      "Physics Device: cuda:1\n",
      "GPU Pipeline: enabled\n",
      "Using SDF cache directory '/home/larsankile/.isaacgym/sdf_V100'\n",
      "~!~!~! Loaded/Cooked SDF triangle mesh 0 @ 0x7f8b89664640, resolution=512, spacing=0.000317\n",
      "  ~!~! Bounds:  (-0.081250, 0.081250) (-0.015685, 0.015565) (-0.081250, 0.081251)\n",
      "  ~!~! Extents: (0.162500, 0.031250, 0.162501)\n",
      "  ~!~! Resolution: (512, 99, 512)\n",
      "~!~!~! Loaded/Cooked SDF triangle mesh 1 @ 0x7f8b88d543b0, resolution=512, spacing=0.000171\n",
      "  ~!~! Bounds:  (-0.015000, 0.015000) (-0.056250, 0.031250) (-0.014383, 0.015618)\n",
      "  ~!~! Extents: (0.030000, 0.087500, 0.030001)\n",
      "  ~!~! Resolution: (176, 512, 176)\n",
      "~!~!~! Loaded/Cooked SDF triangle mesh 2 @ 0x7f8aea4a8450, resolution=512, spacing=0.000172\n",
      "  ~!~! Bounds:  (-0.015000, 0.015000) (-0.056562, 0.031376) (-0.015438, 0.014563)\n",
      "  ~!~! Extents: (0.030000, 0.087938, 0.030001)\n",
      "  ~!~! Resolution: (175, 512, 175)\n",
      "~!~!~! Loaded/Cooked SDF triangle mesh 3 @ 0x7f8aea3b3220, resolution=512, spacing=0.000171\n",
      "  ~!~! Bounds:  (-0.015000, 0.015000) (-0.056250, 0.031250) (-0.014375, 0.015625)\n",
      "  ~!~! Extents: (0.030000, 0.087500, 0.030000)\n",
      "  ~!~! Resolution: (176, 512, 176)\n",
      "~!~!~! Loaded/Cooked SDF triangle mesh 4 @ 0x7f8aeb8beff0, resolution=512, spacing=0.000171\n",
      "  ~!~! Bounds:  (-0.015000, 0.015000) (-0.056250, 0.031250) (-0.015618, 0.014383)\n",
      "  ~!~! Extents: (0.030000, 0.087500, 0.030001)\n",
      "  ~!~! Resolution: (176, 512, 176)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larsankile/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/larsankile/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/larsankile/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"FurnitureSimImageFeature-v0\",\n",
    "    furniture=\"one_leg\",  # Specifies the type of furniture [lamp | square_table | desk | drawer | cabinet | round_table | stool | chair | one_leg].\n",
    "    encoder_type=\"vip\",\n",
    "    num_envs=1,  # Number of parallel environments.\n",
    "    include_raw_images=True,\n",
    "    headless=True,  # If true, simulation runs without GUI.\n",
    "    compute_device_id=1,  # GPU device ID for simulation.\n",
    "    graphics_device_id=1,  # GPU device ID for rendering.\n",
    "    init_assembled=False,  # If true, the environment is initialized with assembled furniture.\n",
    "    randomness=\"low\",  # Level of randomness in the environment [low | med | high].\n",
    "    high_random_idx=-1,  # Index of the high randomness level (range: [0-2]). Default -1 will randomly select the index within the range.\n",
    "    save_camera_input=False,  # If true, the initial camera inputs are saved.\n",
    "    record=False,  # If true, videos of the wrist and front cameras' RGB inputs are recorded.\n",
    "    max_env_steps=3000,  # Maximum number of steps per episode.\n",
    "    act_rot_repr=\"quat\",  # Representation of rotation for action space. Options are 'quat' and 'axis'.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Found collision-free init pose\n",
      "/home/larsankile/miniconda3/envs/rlgpu/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robot state dim: 14, Image features dim 2048, Total obs dim: 2062, Action dim: 8\n",
      "Obs:  {'robot_state': array([ 0.6218,  0.0236,  0.1809, ..., -2.696 , -3.5616,  0.0614],\n",
      "      dtype=float32), 'image1': array([-0.0199, -0.0133,  1.5356, ...,  0.0114, -0.3933,  0.0133],\n",
      "      dtype=float32), 'image2': array([-0.0165, -0.015 ,  0.556 , ...,  0.013 , -0.0332,  0.0134],\n",
      "      dtype=float32), 'color_image1': array([[[128, 128, 129, ..., 156, 156, 156],\n",
      "        [128, 128, 129, ..., 156, 156, 156],\n",
      "        [128, 128, 129, ..., 156, 156, 156],\n",
      "        ...,\n",
      "        [ 26,  26,  26, ...,  26,  26,  26],\n",
      "        [ 26,  26,  26, ...,  26,  26,  26],\n",
      "        [ 26,  26,  26, ...,  26,  26,  26]],\n",
      "\n",
      "       [[128, 128, 129, ..., 156, 156, 156],\n",
      "        [128, 128, 129, ..., 156, 156, 156],\n",
      "        [128, 128, 129, ..., 156, 156, 156],\n",
      "        ...,\n",
      "        [ 26,  26,  26, ...,  26,  26,  26],\n",
      "        [ 26,  26,  26, ...,  26,  26,  26],\n",
      "        [ 26,  26,  26, ...,  26,  26,  26]],\n",
      "\n",
      "       [[126, 126, 127, ..., 156, 156, 156],\n",
      "        [126, 127, 127, ..., 156, 156, 156],\n",
      "        [126, 127, 127, ..., 156, 156, 156],\n",
      "        ...,\n",
      "        [ 26,  26,  26, ...,  26,  26,  26],\n",
      "        [ 26,  26,  26, ...,  26,  26,  26],\n",
      "        [ 26,  26,  26, ...,  26,  26,  26]]], dtype=uint8), 'color_image2': array([[[ 0,  0,  0, ...,  0,  0,  0],\n",
      "        [ 0,  0,  0, ...,  0,  0,  0],\n",
      "        [ 0,  0,  0, ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [42, 42, 42, ..., 42, 42, 42],\n",
      "        [42, 42, 42, ..., 42, 42, 42],\n",
      "        [42, 42, 42, ..., 42, 42, 42]],\n",
      "\n",
      "       [[27, 27, 27, ..., 43, 43, 43],\n",
      "        [27, 27, 27, ..., 43, 43, 43],\n",
      "        [27, 27, 27, ..., 43, 43, 43],\n",
      "        ...,\n",
      "        [42, 42, 42, ..., 42, 42, 42],\n",
      "        [42, 42, 42, ..., 42, 42, 42],\n",
      "        [42, 42, 42, ..., 42, 42, 42]],\n",
      "\n",
      "       [[14, 14, 14, ..., 22, 22, 22],\n",
      "        [14, 14, 14, ..., 22, 22, 22],\n",
      "        [14, 14, 14, ..., 22, 22, 22],\n",
      "        ...,\n",
      "        [41, 41, 41, ..., 41, 41, 41],\n",
      "        [41, 41, 41, ..., 41, 41, 41],\n",
      "        [41, 41, 41, ..., 41, 41, 41]]], dtype=uint8)}\n",
      "Action:  array([[ 0.9773, -0.9746,  0.6358, ...,  0.2594, -0.0569,  0.049 ]],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 1. seed env for initial state.\n",
    "# Seed 0-200 are used for the demonstration dataset.\n",
    "env.seed(1000)\n",
    "\n",
    "# 2. must reset before use\n",
    "obs = env.reset()\n",
    "\n",
    "# 3.\n",
    "action = env.action_space.sample()\n",
    "\n",
    "# 4. Standard gym step method\n",
    "obs, reward, done, info = env.step(action)\n",
    "\n",
    "robot_state_dim = obs[\"robot_state\"].shape[0]\n",
    "\n",
    "image_feature_dim = obs[\"image1\"].shape[0] * 2\n",
    "obs_dim = robot_state_dim + (image_feature_dim)\n",
    "action_dim = action.shape[1]\n",
    "\n",
    "# prints and explains each dimension of the observation and action vectors\n",
    "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
    "    print(\n",
    "        f\"Robot state dim: {robot_state_dim}, Image features dim {image_feature_dim}, Total obs dim: {obs_dim}, Action dim: {action_dim}\"\n",
    "    )\n",
    "    print(\"Obs: \", repr(obs))\n",
    "    print(\"Action: \", repr(action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of state-action pairs: 329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 26/329 [00:00<00:02, 124.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 329/329 [00:02<00:00, 123.97it/s]\n"
     ]
    }
   ],
   "source": [
    "file_paths = glob(\"scripted_sim_demo_feature/**/*.pkl\", recursive=True)\n",
    "n_state_action_pairs = len(file_paths)\n",
    "\n",
    "print(f\"Number of state-action pairs: {n_state_action_pairs}\")\n",
    "\n",
    "observations = []\n",
    "actions = []\n",
    "episode_ends = []\n",
    "\n",
    "end_index = 0\n",
    "for path in tqdm(file_paths):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    for obs, action in zip(data[\"observations\"], data[\"actions\"]):\n",
    "        # Each observation is just a concatenation of the robot state and the object state.\n",
    "        # Collect the robot state.\n",
    "        robot_state = obs[\"robot_state\"]\n",
    "        img1 = obs[\"image1\"]\n",
    "        img2 = obs[\"image2\"]\n",
    "\n",
    "        # import ipdb\n",
    "\n",
    "        # ipdb.set_trace()\n",
    "\n",
    "        # Add the observation to the overall list.\n",
    "        observation = np.concatenate((robot_state, img1, img2))\n",
    "        observations.append(observation)\n",
    "\n",
    "        # Add the action to the overall list.\n",
    "        actions.append(action)\n",
    "\n",
    "        # Increment the end index.\n",
    "        end_index += 1\n",
    "\n",
    "    # Add the end index to the overall list.\n",
    "    episode_ends.append(end_index)\n",
    "\n",
    "# Convert the lists to numpy arrays.\n",
    "observations = np.array(observations)\n",
    "actions = np.array(actions)\n",
    "episode_ends = np.array(episode_ends)\n",
    "\n",
    "# Save the data to a zarr file.\n",
    "zarr.save(\n",
    "    \"demos_feature.zarr\",\n",
    "    observations=observations,\n",
    "    actions=actions,\n",
    "    episode_ends=episode_ends,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159323, 2062), (159323, 8), (329,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_root = zarr.open(\"demos_feature.zarr\", mode=\"r\")\n",
    "\n",
    "dataset_root[\"observations\"].shape, dataset_root[\"actions\"].shape, dataset_root[\n",
    "    \"episode_ends\"\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484.2644376899696"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_root[\"observations\"].shape[0] / dataset_root[\"episode_ends\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   469,    929,   1395,   1933,   2396,   2864,   3389,   3896,\n",
       "         4389,   4867,   5308,   5752,   6188,   6670,   7159,   7637,\n",
       "         8160,   8602,   9141,   9650,  10117,  10610,  11096,  11585,\n",
       "        12044,  12517,  12975,  13480,  13996,  14529,  15051,  15521,\n",
       "        16051,  16552,  17104,  17636,  18135,  18616,  19135,  19634,\n",
       "        20107,  20623,  21150,  21676,  22190,  22697,  23209,  23663,\n",
       "        24191,  24659,  25144,  25675,  26187,  26690,  27156,  27650,\n",
       "        28079,  28562,  29036,  29545,  30030,  30538,  31017,  31478,\n",
       "        31922,  32466,  32921,  33429,  33873,  34335,  34807,  35307,\n",
       "        35796,  36260,  36740,  37263,  37723,  38204,  38687,  39184,\n",
       "        39658,  40163,  40665,  41115,  41623,  42092,  42620,  43074,\n",
       "        43570,  44062,  44583,  45039,  45497,  46006,  46471,  46952,\n",
       "        47420,  47868,  48385,  48899,  49411,  49942,  50396,  50891,\n",
       "        51378,  51851,  52327,  52843,  53363,  53886,  54353,  54841,\n",
       "        55392,  55845,  56323,  56778,  57259,  57752,  58262,  58699,\n",
       "        59161,  59624,  60078,  60561,  61094,  61572,  62035,  62496,\n",
       "        62973,  63501,  63994,  64536,  65018,  65543,  65987,  66455,\n",
       "        66922,  67387,  67921,  68426,  68928,  69414,  69941,  70413,\n",
       "        70892,  71364,  71820,  72285,  72744,  73251,  73803,  74322,\n",
       "        74787,  75324,  75801,  76275,  76735,  77192,  77702,  78165,\n",
       "        78600,  79064,  79532,  80000,  80480,  81019,  81491,  81981,\n",
       "        82436,  82953,  83478,  83969,  84410,  84883,  85365,  85869,\n",
       "        86399,  86882,  87379,  87863,  88317,  88765,  89229,  89729,\n",
       "        90259,  90775,  91244,  91693,  92167,  92694,  93151,  93604,\n",
       "        94117,  94649,  95110,  95583,  96112,  96593,  97041,  97536,\n",
       "        97995,  98459,  98963,  99535, 100025, 100486, 100938, 101466,\n",
       "       101935, 102457, 102912, 103439, 103895, 104416, 104855, 105318,\n",
       "       105839, 106342, 106811, 107323, 107783, 108255, 108721, 109187,\n",
       "       109667, 110140, 110625, 111121, 111579, 112056, 112549, 113014,\n",
       "       113457, 114004, 114447, 114923, 115396, 115912, 116350, 116820,\n",
       "       117275, 117730, 118218, 118735, 119238, 119730, 120212, 120677,\n",
       "       121186, 121713, 122184, 122642, 123152, 123698, 124167, 124693,\n",
       "       125145, 125637, 126102, 126560, 127021, 127496, 127964, 128424,\n",
       "       128897, 129367, 129843, 130315, 130783, 131278, 131802, 132269,\n",
       "       132737, 133266, 133729, 134221, 134681, 135199, 135607, 136085,\n",
       "       136596, 137070, 137539, 137997, 138469, 138957, 139409, 139878,\n",
       "       140350, 140805, 141280, 141752, 142223, 142709, 143176, 143671,\n",
       "       144190, 144674, 145149, 145604, 146107, 146556, 147045, 147488,\n",
       "       147962, 148446, 148914, 149391, 149932, 150416, 150882, 151342,\n",
       "       151817, 152268, 152711, 153185, 153638, 154097, 154553, 155036,\n",
       "       155516, 156006, 156470, 156970, 157436, 157909, 158395, 158848,\n",
       "       159323])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_ends = dataset_root[\"episode_ends\"][:]\n",
    "episode_ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_indices(\n",
    "    episode_ends: np.ndarray,\n",
    "    sequence_length: int,\n",
    "    pad_before: int = 0,\n",
    "    pad_after: int = 0,\n",
    "):\n",
    "    indices = list()\n",
    "    for i in range(len(episode_ends)):\n",
    "        start_idx = 0\n",
    "        if i > 0:\n",
    "            start_idx = episode_ends[i - 1]\n",
    "        end_idx = episode_ends[i]\n",
    "        episode_length = end_idx - start_idx\n",
    "\n",
    "        min_start = -pad_before\n",
    "        max_start = episode_length - sequence_length + pad_after\n",
    "\n",
    "        # range stops one idx before end\n",
    "        for idx in range(min_start, max_start + 1):\n",
    "            buffer_start_idx = max(idx, 0) + start_idx\n",
    "            buffer_end_idx = min(idx + sequence_length, episode_length) + start_idx\n",
    "            start_offset = buffer_start_idx - (idx + start_idx)\n",
    "            end_offset = (idx + sequence_length + start_idx) - buffer_end_idx\n",
    "            sample_start_idx = 0 + start_offset\n",
    "            sample_end_idx = sequence_length - end_offset\n",
    "            indices.append(\n",
    "                [buffer_start_idx, buffer_end_idx, sample_start_idx, sample_end_idx]\n",
    "            )\n",
    "    indices = np.array(indices)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def sample_sequence(\n",
    "    train_data,\n",
    "    sequence_length,\n",
    "    buffer_start_idx,\n",
    "    buffer_end_idx,\n",
    "    sample_start_idx,\n",
    "    sample_end_idx,\n",
    "):\n",
    "    result = dict()\n",
    "    for key, input_arr in train_data.items():\n",
    "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
    "        data = sample\n",
    "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
    "            data = np.zeros(\n",
    "                shape=(sequence_length,) + input_arr.shape[1:], dtype=input_arr.dtype\n",
    "            )\n",
    "            if sample_start_idx > 0:\n",
    "                data[:sample_start_idx] = sample[0]\n",
    "            if sample_end_idx < sequence_length:\n",
    "                data[sample_end_idx:] = sample[-1]\n",
    "            data[sample_start_idx:sample_end_idx] = sample\n",
    "        result[key] = data\n",
    "    return result\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def get_data_stats(data):\n",
    "    data = data.reshape(-1, data.shape[-1])\n",
    "    stats = {\"min\": np.min(data, axis=0), \"max\": np.max(data, axis=0)}\n",
    "    return stats\n",
    "\n",
    "\n",
    "def normalize_data(data, stats):\n",
    "    # nomalize to [0,1]\n",
    "    ndata = (data - stats[\"min\"]) / (stats[\"max\"] - stats[\"min\"])\n",
    "    # normalize to [-1, 1]\n",
    "    ndata = ndata * 2 - 1\n",
    "    return ndata\n",
    "\n",
    "\n",
    "def unnormalize_data(ndata, stats):\n",
    "    ndata = (ndata + 1) / 2\n",
    "    data = ndata * (stats[\"max\"] - stats[\"min\"]) + stats[\"min\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "# dataset\n",
    "class OneLegStateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, pred_horizon, obs_horizon, action_horizon):\n",
    "        # read from zarr dataset\n",
    "        dataset = zarr.open(dataset_path, \"r\")\n",
    "        # All demonstration episodes are concatinated in the first dimension N\n",
    "        train_data = {\n",
    "            # (N, action_dim)\n",
    "            \"action\": dataset[\"actions\"][:],\n",
    "            # (N, obs_dim)\n",
    "            \"obs\": dataset[\"observations\"][:],\n",
    "        }\n",
    "        # Marks one-past the last index for each episode\n",
    "        episode_ends = dataset[\"episode_ends\"][:]\n",
    "\n",
    "        # compute start and end of each state-action sequence\n",
    "        # also handles padding\n",
    "        indices = create_sample_indices(\n",
    "            episode_ends=episode_ends,\n",
    "            sequence_length=pred_horizon,\n",
    "            # add padding such that each timestep in the dataset are seen\n",
    "            pad_before=obs_horizon - 1,\n",
    "            pad_after=action_horizon - 1,\n",
    "        )\n",
    "\n",
    "        # compute statistics and normalized data to [-1,1]\n",
    "        stats = dict()\n",
    "        normalized_train_data = dict()\n",
    "        for key, data in train_data.items():\n",
    "            stats[key] = get_data_stats(data)\n",
    "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
    "\n",
    "        self.indices = indices\n",
    "        self.stats = stats\n",
    "        self.normalized_train_data = normalized_train_data\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.action_horizon = action_horizon\n",
    "        self.obs_horizon = obs_horizon\n",
    "\n",
    "        # Set action and observation dimensions\n",
    "        self.action_dim = train_data[\"action\"].shape[-1]\n",
    "        self.obs_dim = train_data[\"obs\"].shape[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        # all possible segments of the dataset\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the start/end indices for this datapoint\n",
    "        (\n",
    "            buffer_start_idx,\n",
    "            buffer_end_idx,\n",
    "            sample_start_idx,\n",
    "            sample_end_idx,\n",
    "        ) = self.indices[idx]\n",
    "\n",
    "        # get normalized data using these indices\n",
    "        nsample = sample_sequence(\n",
    "            train_data=self.normalized_train_data,\n",
    "            sequence_length=self.pred_horizon,\n",
    "            buffer_start_idx=buffer_start_idx,\n",
    "            buffer_end_idx=buffer_end_idx,\n",
    "            sample_start_idx=sample_start_idx,\n",
    "            sample_end_idx=sample_end_idx,\n",
    "        )\n",
    "\n",
    "        # discard unused observations\n",
    "        nsample[\"obs\"] = nsample[\"obs\"][: self.obs_horizon, :]\n",
    "        return nsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = OneLegStateDataset(\n",
    "    dataset_path=\"../demos.zarr\",\n",
    "    pred_horizon=20,\n",
    "    obs_horizon=2,\n",
    "    action_horizon=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 49)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.action_dim, d.obs_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 49), (20, 8))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0][\"obs\"].shape, d[0][\"action\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Downsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Upsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv1d --> GroupNorm --> Mish\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                inp_channels, out_channels, kernel_size, padding=kernel_size // 2\n",
    "            ),\n",
    "            nn.GroupNorm(n_groups, out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, cond_dim, kernel_size=3, n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "                Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
    "        # predicts per-channel scale and bias\n",
    "        cond_channels = out_channels * 2\n",
    "        self.out_channels = out_channels\n",
    "        self.cond_encoder = nn.Sequential(\n",
    "            nn.Mish(), nn.Linear(cond_dim, cond_channels), nn.Unflatten(-1, (-1, 1))\n",
    "        )\n",
    "\n",
    "        # make sure dimensions compatible\n",
    "        self.residual_conv = (\n",
    "            nn.Conv1d(in_channels, out_channels, 1)\n",
    "            if in_channels != out_channels\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        \"\"\"\n",
    "        x : [ batch_size x in_channels x horizon ]\n",
    "        cond : [ batch_size x cond_dim]\n",
    "\n",
    "        returns:\n",
    "        out : [ batch_size x out_channels x horizon ]\n",
    "        \"\"\"\n",
    "        out = self.blocks[0](x)\n",
    "        embed = self.cond_encoder(cond)\n",
    "\n",
    "        embed = embed.reshape(embed.shape[0], 2, self.out_channels, 1)\n",
    "        scale = embed[:, 0, ...]\n",
    "        bias = embed[:, 1, ...]\n",
    "        out = scale * out + bias\n",
    "\n",
    "        out = self.blocks[1](out)\n",
    "        out = out + self.residual_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalUnet1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        global_cond_dim,\n",
    "        diffusion_step_embed_dim=256,\n",
    "        down_dims=[256, 512, 1024],\n",
    "        kernel_size=5,\n",
    "        n_groups=8,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_dim: Dim of actions.\n",
    "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
    "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
    "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
    "        down_dims: Channel size for each UNet level.\n",
    "          The length of this array determines numebr of levels.\n",
    "        kernel_size: Conv kernel size\n",
    "        n_groups: Number of groups for GroupNorm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        all_dims = [input_dim] + list(down_dims)\n",
    "        start_dim = down_dims[0]\n",
    "\n",
    "        dsed = diffusion_step_embed_dim\n",
    "        diffusion_step_encoder = nn.Sequential(\n",
    "            SinusoidalPosEmb(dsed),\n",
    "            nn.Linear(dsed, dsed * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(dsed * 4, dsed),\n",
    "        )\n",
    "        cond_dim = dsed + global_cond_dim\n",
    "\n",
    "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
    "        mid_dim = all_dims[-1]\n",
    "        self.mid_modules = nn.ModuleList(\n",
    "            [\n",
    "                ConditionalResidualBlock1D(\n",
    "                    mid_dim,\n",
    "                    mid_dim,\n",
    "                    cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size,\n",
    "                    n_groups=n_groups,\n",
    "                ),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    mid_dim,\n",
    "                    mid_dim,\n",
    "                    cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size,\n",
    "                    n_groups=n_groups,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        down_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            down_modules.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        ConditionalResidualBlock1D(\n",
    "                            dim_in,\n",
    "                            dim_out,\n",
    "                            cond_dim=cond_dim,\n",
    "                            kernel_size=kernel_size,\n",
    "                            n_groups=n_groups,\n",
    "                        ),\n",
    "                        ConditionalResidualBlock1D(\n",
    "                            dim_out,\n",
    "                            dim_out,\n",
    "                            cond_dim=cond_dim,\n",
    "                            kernel_size=kernel_size,\n",
    "                            n_groups=n_groups,\n",
    "                        ),\n",
    "                        Downsample1d(dim_out) if not is_last else nn.Identity(),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        up_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            up_modules.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        ConditionalResidualBlock1D(\n",
    "                            dim_out * 2,\n",
    "                            dim_in,\n",
    "                            cond_dim=cond_dim,\n",
    "                            kernel_size=kernel_size,\n",
    "                            n_groups=n_groups,\n",
    "                        ),\n",
    "                        ConditionalResidualBlock1D(\n",
    "                            dim_in,\n",
    "                            dim_in,\n",
    "                            cond_dim=cond_dim,\n",
    "                            kernel_size=kernel_size,\n",
    "                            n_groups=n_groups,\n",
    "                        ),\n",
    "                        Upsample1d(dim_in) if not is_last else nn.Identity(),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        final_conv = nn.Sequential(\n",
    "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
    "            nn.Conv1d(start_dim, input_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.diffusion_step_encoder = diffusion_step_encoder\n",
    "        self.up_modules = up_modules\n",
    "        self.down_modules = down_modules\n",
    "        self.final_conv = final_conv\n",
    "\n",
    "        print(\n",
    "            \"number of parameters: {:e}\".format(\n",
    "                sum(p.numel() for p in self.parameters())\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.Tensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "        global_cond=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        x: (B,T,input_dim)\n",
    "        timestep: (B,) or int, diffusion step\n",
    "        global_cond: (B,global_cond_dim)\n",
    "        output: (B,T,input_dim)\n",
    "        \"\"\"\n",
    "        # (B,T,C)\n",
    "        sample = sample.moveaxis(-1, -2)\n",
    "        # (B,C,T)\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            timesteps = torch.tensor(\n",
    "                [timesteps], dtype=torch.long, device=sample.device\n",
    "            )\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        global_feature = self.diffusion_step_encoder(timesteps)\n",
    "\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([global_feature, global_cond], axis=-1)\n",
    "\n",
    "        x = sample\n",
    "        h = []\n",
    "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        for mid_module in self.mid_modules:\n",
    "            x = mid_module(x, global_feature)\n",
    "\n",
    "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # (B,C,T)\n",
    "        x = x.moveaxis(-1, -2)\n",
    "        # (B,T,C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Damn Thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a seed >200 to avoid initial states seen in the training dataset\n",
    "import collections\n",
    "\n",
    "\n",
    "def rollout(\n",
    "    env,\n",
    "    noise_pred_net,\n",
    "    stats,\n",
    "    inference_steps,\n",
    "    config,\n",
    "    max_steps=500,\n",
    "):\n",
    "    # env.seed(10_000)\n",
    "\n",
    "    noise_scheduler = DDIMScheduler(\n",
    "        num_train_timesteps=config.num_diffusion_iters,\n",
    "        beta_schedule=config.beta_schedule,\n",
    "        clip_sample=config.clip_sample,\n",
    "        prediction_type=config.prediction_type,\n",
    "    )\n",
    "\n",
    "    # get first observation\n",
    "    obs = env.reset()\n",
    "\n",
    "    # keep a queue of last 2 steps of observations\n",
    "    obs_deque = collections.deque(\n",
    "        [torch.cat([obs[\"robot_state\"], obs[\"parts_poses\"]], dim=-1).cpu()]\n",
    "        * config.obs_horizon,\n",
    "        maxlen=config.obs_horizon,\n",
    "    )\n",
    "    # save visualization and rewards\n",
    "    imgs1 = [obs[\"color_image1\"]]\n",
    "    imgs2 = [obs[\"color_image2\"]]\n",
    "    rewards = list()\n",
    "    done = False\n",
    "    step_idx = 0\n",
    "\n",
    "    with tqdm(total=max_steps, desc=\"Eval OneLeg State Env\") as pbar:\n",
    "        while not done:\n",
    "            B = 1\n",
    "            # stack the last obs_horizon (2) number of observations\n",
    "            obs_seq = np.stack(obs_deque)\n",
    "            # normalize observation\n",
    "\n",
    "            nobs = normalize_data(obs_seq, stats=stats[\"obs\"])\n",
    "            # device transfer\n",
    "            nobs = torch.from_numpy(nobs).to(device, dtype=torch.float32)\n",
    "\n",
    "            # infer action\n",
    "            with torch.no_grad():\n",
    "                # reshape observation to (B,obs_horizon*obs_dim)\n",
    "                obs_cond = nobs.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "                # initialize action from Guassian noise\n",
    "                noisy_action = torch.randn(\n",
    "                    (B, config.pred_horizon, action_dim), device=device\n",
    "                )\n",
    "                naction = noisy_action\n",
    "\n",
    "                # init scheduler\n",
    "                noise_scheduler.set_timesteps(inference_steps)\n",
    "\n",
    "                for k in noise_scheduler.timesteps:\n",
    "                    # predict noise\n",
    "                    noise_pred = noise_pred_net(\n",
    "                        sample=naction, timestep=k, global_cond=obs_cond\n",
    "                    )\n",
    "\n",
    "                    # inverse diffusion step (remove noise)\n",
    "                    naction = noise_scheduler.step(\n",
    "                        model_output=noise_pred, timestep=k, sample=naction\n",
    "                    ).prev_sample\n",
    "\n",
    "            # unnormalize action\n",
    "            naction = naction.detach().to(\"cpu\").numpy()\n",
    "            # (B, pred_horizon, action_dim)\n",
    "            naction = naction[0]\n",
    "            action_pred = unnormalize_data(naction, stats=stats[\"action\"])\n",
    "\n",
    "            # only take action_horizon number of actions\n",
    "            start = config.obs_horizon - 1\n",
    "            end = start + config.action_horizon\n",
    "            action = action_pred[start:end, :]\n",
    "            # (action_horizon, action_dim)\n",
    "\n",
    "            # execute action_horizon number of steps\n",
    "            # without replanning\n",
    "            for i in range(len(action)):\n",
    "                # stepping env\n",
    "                obs, reward, done, info = env.step(action[i])\n",
    "                # save observations\n",
    "                obs_deque.append(\n",
    "                    torch.cat([obs[\"robot_state\"], obs[\"parts_poses\"]], dim=-1).cpu()\n",
    "                )\n",
    "                # and reward/vis\n",
    "                rewards.append(reward.cpu().item())\n",
    "                imgs1.append(obs[\"color_image1\"])\n",
    "                imgs2.append(obs[\"color_image2\"])\n",
    "\n",
    "                # update progress bar\n",
    "                step_idx += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(reward=reward)\n",
    "                if step_idx > max_steps:\n",
    "                    done = True\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "    return rewards, imgs1, imgs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_mp4(ims1, ims2, filename=None):\n",
    "    # Initialize plot with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    # Function to update plot\n",
    "    def update(num):\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        ax1.axis(\"off\")\n",
    "        ax2.axis(\"off\")\n",
    "\n",
    "        img_array1 = ims1[num]\n",
    "        if isinstance(img_array1, torch.Tensor):\n",
    "            img_array1 = img_array1.squeeze(0).cpu().numpy()\n",
    "\n",
    "        img_array2 = ims2[num]\n",
    "        if isinstance(img_array2, torch.Tensor):\n",
    "            img_array2 = img_array2.squeeze(0).cpu().numpy()\n",
    "\n",
    "        ax1.imshow(img_array1)\n",
    "        ax2.imshow(img_array2)\n",
    "\n",
    "    frame_indices = range(0, len(ims1), 1)\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=tqdm(frame_indices), interval=100)\n",
    "\n",
    "    if not filename:\n",
    "        filename = f\"render-{datetime.now()}.mp4\"\n",
    "\n",
    "    ani.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mankile\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/larsankile/furniture-diffusion/wandb/run-20230920_211240-rd0fepm8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ankile/furniture-diffusion/runs/rd0fepm8' target=\"_blank\">rare-terrain-14</a></strong> to <a href='https://wandb.ai/ankile/furniture-diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ankile/furniture-diffusion' target=\"_blank\">https://wandb.ai/ankile/furniture-diffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ankile/furniture-diffusion/runs/rd0fepm8' target=\"_blank\">https://wandb.ai/ankile/furniture-diffusion/runs/rd0fepm8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.591427e+08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/100 [00:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/larsankile/furniture-diffusion/behavioral_cloning_state.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bimprobable-lambda.csail.mit.edu/home/larsankile/furniture-diffusion/behavioral_cloning_state.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=138'>139</a>\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bimprobable-lambda.csail.mit.edu/home/larsankile/furniture-diffusion/behavioral_cloning_state.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39m# update Exponential Moving Average of the model weights\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bimprobable-lambda.csail.mit.edu/home/larsankile/furniture-diffusion/behavioral_cloning_state.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=141'>142</a>\u001b[0m \u001b[39m# ema.step(noise_pred_net.parameters())\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bimprobable-lambda.csail.mit.edu/home/larsankile/furniture-diffusion/behavioral_cloning_state.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bimprobable-lambda.csail.mit.edu/home/larsankile/furniture-diffusion/behavioral_cloning_state.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=143'>144</a>\u001b[0m \u001b[39m# logging\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bimprobable-lambda.csail.mit.edu/home/larsankile/furniture-diffusion/behavioral_cloning_state.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m loss_cpu \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bimprobable-lambda.csail.mit.edu/home/larsankile/furniture-diffusion/behavioral_cloning_state.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=145'>146</a>\u001b[0m epoch_loss\u001b[39m.\u001b[39mappend(loss_cpu)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bimprobable-lambda.csail.mit.edu/home/larsankile/furniture-diffusion/behavioral_cloning_state.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m wandb\u001b[39m.\u001b[39mlog({\u001b[39m\"\u001b[39m\u001b[39mbatch_loss\u001b[39m\u001b[39m\"\u001b[39m: loss_cpu})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# config for wandb\n",
    "config = dict(\n",
    "    pred_horizon=16,\n",
    "    obs_horizon=2,\n",
    "    action_horizon=4,\n",
    "    down_dims=[256, 512, 1024, 2048],\n",
    "    batch_size=1024,\n",
    "    num_epochs=100,\n",
    "    num_diffusion_iters=100,\n",
    "    beta_schedule=\"squaredcos_cap_v2\",\n",
    "    clip_sample=True,\n",
    "    prediction_type=\"epsilon\",\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-6,\n",
    "    ema_power=0.75,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    lr_scheduler_warmup_steps=500,\n",
    "    dataloader_workers=4,\n",
    "    rollout_every=10,\n",
    "    inference_steps=10,\n",
    "    ema_model=False,\n",
    ")\n",
    "\n",
    "# Init wandb\n",
    "wandb.init(project=\"furniture-diffusion\", entity=\"ankile\", config=config)\n",
    "config = wandb.config\n",
    "\n",
    "dataset = OneLegStateDataset(\n",
    "    dataset_path=\"demos.zarr\",\n",
    "    pred_horizon=config.pred_horizon,\n",
    "    obs_horizon=config.obs_horizon,\n",
    "    action_horizon=config.action_horizon,\n",
    ")\n",
    "\n",
    "# save training data statistics (min, max) for each dim\n",
    "stats = dataset.stats\n",
    "\n",
    "# save stats to wandb\n",
    "wandb.log(stats)\n",
    "\n",
    "# create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.dataloader_workers,\n",
    "    shuffle=True,\n",
    "    # accelerate cpu-gpu transfer\n",
    "    pin_memory=True,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim * config.obs_horizon,\n",
    "    down_dims=config.down_dims,\n",
    ").to(device)\n",
    "\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=config.num_diffusion_iters,\n",
    "    # the choise of beta schedule has big impact on performance\n",
    "    # we found squared cosine works the best\n",
    "    beta_schedule=config.beta_schedule,\n",
    "    # clip output to [-1,1] to improve stability\n",
    "    clip_sample=config.clip_sample,\n",
    "    # our network predicts noise (instead of denoised action)\n",
    "    prediction_type=config.prediction_type,\n",
    ")\n",
    "\n",
    "wandb.watch(noise_pred_net)\n",
    "\n",
    "# Exponential Moving Average\n",
    "# accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "# ema = EMAModel(parameters=noise_pred_net.parameters(), power=config.ema_power)\n",
    "\n",
    "# AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=noise_pred_net.parameters(),\n",
    "    lr=config.lr,\n",
    "    weight_decay=config.weight_decay,\n",
    ")\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=config.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_scheduler_warmup_steps,\n",
    "    num_training_steps=len(dataloader) * config.num_epochs,\n",
    ")\n",
    "\n",
    "tglobal = tqdm(range(config.num_epochs), desc=\"Epoch\")\n",
    "\n",
    "# epoch loop\n",
    "for epoch_idx in tglobal:\n",
    "    epoch_loss = list()\n",
    "    # batch loop\n",
    "    with tqdm(dataloader, desc=\"Batch\", leave=False) as tepoch:\n",
    "        for nbatch in tepoch:\n",
    "            # data normalized in dataset\n",
    "            # device transfer\n",
    "            nobs = nbatch[\"obs\"].to(device)\n",
    "            naction = nbatch[\"action\"].to(device)\n",
    "            B = nobs.shape[0]\n",
    "\n",
    "            # observation as FiLM conditioning\n",
    "            # (B, obs_horizon, obs_dim)\n",
    "            obs_cond = nobs[:, : config.obs_horizon, :]\n",
    "            # (B, obs_horizon * obs_dim)\n",
    "            obs_cond = obs_cond.flatten(start_dim=1)\n",
    "\n",
    "            # sample noise to add to actions\n",
    "            noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "            # sample a diffusion iteration for each data point\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (B,), device=device\n",
    "            ).long()\n",
    "\n",
    "            # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_actions = noise_scheduler.add_noise(naction, noise, timesteps)\n",
    "\n",
    "            # predict the noise residual\n",
    "            noise_pred = noise_pred_net(noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "            # L2 loss\n",
    "            loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "            # optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # step lr scheduler every batch\n",
    "            # this is different from standard pytorch behavior\n",
    "            wandb.log({\"lr\": lr_scheduler.get_last_lr()[0]})\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # update Exponential Moving Average of the model weights\n",
    "            # ema.step(noise_pred_net.parameters())\n",
    "\n",
    "            # logging\n",
    "            loss_cpu = loss.item()\n",
    "            epoch_loss.append(loss_cpu)\n",
    "            wandb.log({\"batch_loss\": loss_cpu})\n",
    "\n",
    "            tepoch.set_postfix(loss=loss_cpu)\n",
    "    tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
    "    wandb.log({\"epoch_loss\": np.mean(epoch_loss), \"epoch\": epoch_idx})\n",
    "\n",
    "    if epoch_idx % config.rollout_every == 0:\n",
    "        # save model every 10 epochs\n",
    "        torch.save(\n",
    "            noise_pred_net.state_dict(),\n",
    "            f\"noise_pred_net{wandb.run.name}-{epoch_idx}.pt\",\n",
    "        )\n",
    "        wandb.save(f\"noise_pred_net{wandb.run.name}-{epoch_idx}.pt\")\n",
    "\n",
    "        # Perform a rollout with the current model\n",
    "        rewards, imgs1, imgs2 = rollout(\n",
    "            env,\n",
    "            noise_pred_net,\n",
    "            stats,\n",
    "            config,\n",
    "            inference_steps=config.inference_steps,\n",
    "        )\n",
    "\n",
    "        # Make a video of the rollout\n",
    "        filename = f\"rollout-{wandb.run.name}-{epoch_idx}.mp4\"\n",
    "        render_mp4(imgs1, imgs2, filename=filename)\n",
    "\n",
    "        # Log the video to wandb\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"rollout\": wandb.Video(\n",
    "                    filename,\n",
    "                    fps=10,\n",
    "                    format=\"mp4\",\n",
    "                    caption=f\"Epoch {epoch_idx}\",\n",
    "                ),\n",
    "                \"epoch\": epoch_idx,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "tglobal.close()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
